{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d40df1-33d4-4845-8100-7b15f96be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from helpers.helper_functions import translate_to_file_string\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb235b29-0dc0-4efa-b572-63dbc88bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/Flight_Delay_Jan_2020_ontime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a794193-0fee-4f5b-ab7d-d04f71168efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"FlightDelay\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81c97e-929d-49db-915a-b88d5cf4e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(inputFile)\n",
    "\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb783a-d512-49ff-afb8-145b852ff68b",
   "metadata": {},
   "source": [
    "### Remove faulty features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd1f21-4dd2-443a-80ee-2d9a4be1cbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pysparkDF = pysparkDF.drop('_c21')\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d069aa-0c9c-4bec-87b6-fee149db57b1",
   "metadata": {},
   "source": [
    "### Remove records containing NULLvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1ce3-5ff6-4521-be22-2acafffd4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF_nonull = pysparkDF.dropna()\n",
    "f\"Removed {pysparkDF.count()-pysparkDF_nonull.count()} records containing NULL values\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4059e-49e5-452e-b592-a2b3465cab47",
   "metadata": {},
   "source": [
    "### Build String indexer for TAIL_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff308-39fc-490f-8416-e18fef441861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailNum_Indexer = StringIndexer().setInputCol(\"TAIL_NUM\").setOutputCol(\"TAIL_NUM_ID\").fit(pysparkDF_nonull)\n",
    "pysparkDF_indexed = tailNum_Indexer.transform(pysparkDF_nonull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8baf3-2876-4edb-ad65-718dc0c8fd7a",
   "metadata": {},
   "source": [
    "### Define label columns and remove non-BOOL label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcfd64-4fa0-4f76-b0cf-f61d71bfb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCols = [\"DEP_DEL15\",\"ARR_DEL15\"]\n",
    "labelCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71add9-035e-4e21-8603-3f4e49e989ed",
   "metadata": {},
   "source": [
    "### Get weight values for features realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c5025-32f9-4847-bafb-2cb1b633b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedDF = pysparkDF_indexed\n",
    "\n",
    "for label in [\"DEP_DEL15\",\"ARR_DEL15\"]:\n",
    "    delayedDf = pysparkDF_indexed.filter(label + \"=1.0\")\n",
    "    sampleRatio = delayedDf.count() / pysparkDF_indexed.count()\n",
    "\n",
    "    ratioOfDelayed = sampleRatio\n",
    "    delayedWeight  = 1 - ratioOfDelayed\n",
    "    nonDelayedWeight = ratioOfDelayed\n",
    "\n",
    "    weightedDF = weightedDF.withColumn(label + \"_weighted\", F.when(weightedDF[label]==(\"1.0\"),delayedWeight).otherwise(nonDelayedWeight))\n",
    "\n",
    "weightedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea5e71-bb8c-44bf-b058-6b0bd155795b",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for unconditional prediction\n",
    "-> Unconditional is referring to predicting each of the labels without having information on the current status of the flight (Use-Case: Checking the day before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13657d-d85e-4833-b671-dfcacbd418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_unconditional = weightedDF.columns.copy()\n",
    "featureCols_unconditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_unconditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in labelCols:\n",
    "    featureCols_unconditional.remove(label)\n",
    "featureCols_unconditional.remove(\"CANCELLED\")\n",
    "featureCols_unconditional.remove(\"DIVERTED\")\n",
    "featureCols_unconditional.remove(\"DEP_DEL15_weighted\")\n",
    "featureCols_unconditional.remove(\"ARR_DEL15_weighted\")\n",
    "                                     \n",
    "featureCols_unconditional                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae89514-182f-4972-a212-e73435c11ab7",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for conditional prediction\n",
    "-> Conditional is referring to predicting each of the labels considering available real-time information on the current status of the flight (Use-Case: Checking while at the airport, pre-flight)\n",
    "\n",
    "One would expect that prediction performance is increased when the model is aware of the current flight status (=DEP_DEL15)\n",
    "\n",
    "Example: If the model is aware that the flight has departure delay, it might be able to better predict whether it will also be delayed at arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30055325-c3d0-43ed-890a-9ecc0e78d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_conditional = weightedDF.columns.copy()\n",
    "featureCols_conditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_conditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in [label for label in labelCols if label!=\"DEP_DEL15\"]:\n",
    "    featureCols_conditional.remove(label)\n",
    "featureCols_conditional.remove(\"CANCELLED\")\n",
    "featureCols_conditional.remove(\"DIVERTED\")\n",
    "featureCols_conditional.remove(\"DEP_DEL15_weighted\")\n",
    "featureCols_conditional.remove(\"ARR_DEL15_weighted\")\n",
    "    \n",
    "featureCols_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e42ee-540f-46e6-8058-a43e2127d260",
   "metadata": {},
   "source": [
    "### Build and apply feature column assembler for both featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7bff2-24d5-4d7e-a5e9-64cf2e753a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_unconditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_unconditional))\n",
    "assembler_conditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_conditional))\n",
    "\n",
    "featureSet_unconditional = assembler_unconditional.transform(weightedDF)\n",
    "featureSet_conditional = assembler_conditional.transform(weightedDF)\n",
    "\n",
    "# Define same base-scaler for both feature cols\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, \n",
    "                        withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel_unconditional = scaler.fit(featureSet_unconditional)\n",
    "scalerModel_conditional = scaler.fit(featureSet_conditional)\n",
    "\n",
    "scaledFeatureSet_unconditional = scalerModel_unconditional.transform(featureSet_unconditional)\n",
    "scaledFeatureSet_conditional = scalerModel_conditional.transform(featureSet_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a406d7a-4fdc-4e57-a2c1-27fb1d8bc5f7",
   "metadata": {},
   "source": [
    "### Split data into training and test set\n",
    "Die Aufteilung der Daten erfolgt in 80% Trainingsdaten und 20% Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3687a6b-fbb2-4250-8282-4aa6cd61f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_unconditional = scaledFeatureSet_unconditional.randomSplit([0.8, 0.2], 12345)\n",
    "training_unconditional = splits_unconditional[0]\n",
    "test_unconditional = splits_unconditional[1]\n",
    "print('Count train data unconditional: ' + str(training_unconditional.count()))\n",
    "print('Count test data unconditional: ' + str(test_unconditional.count()))\n",
    "\n",
    "\n",
    "splits_conditional= scaledFeatureSet_conditional.randomSplit([0.8, 0.2], 12345)\n",
    "training_conditional = splits_conditional[0]\n",
    "test_conditional = splits_conditional[1]\n",
    "print('Count train data conditional: ' + str(training_conditional.count()))\n",
    "print('Count test data conditional: ' + str(test_conditional.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b49f5-d45a-43f3-b37a-3c0a11e6db85",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "- Logistische Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7aeee8-4351-4ff6-a103-f69154c5c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression,LinearSVC,GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6fdf5-fc11-4871-87a4-e2f5f22eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "      StructField('model', StringType(), True),\n",
    "      StructField('target label', StringType(), True),\n",
    "      StructField('mode', StringType(), True),\n",
    "      StructField('param_config', StringType(), True),\n",
    "      StructField('accuracy', FloatType(), True),\n",
    "      StructField('fmeasure', FloatType(), True),\n",
    "      StructField('test error', FloatType(), True)\n",
    "  ])\n",
    "\n",
    "evalDF = spark.createDataFrame([], schema)\n",
    "evalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8fedb-25a4-4767-bcc1-bbfb5de58123",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"LogisticRegression\",\"SVM\"]\n",
    "\n",
    "for model in models:\n",
    "    for label in labelCols:\n",
    "        for test, train, mode, features in zip([test_unconditional, test_conditional],[training_unconditional,training_conditional],[\"Unconditional\",\"Conditional\"],[featureCols_unconditional,featureCols_conditional]):\n",
    "            # Skip invalid combinations\n",
    "            if label==\"DEP_DEL15\" and mode==\"Conditional\":\n",
    "                continue\n",
    "\n",
    "            # Print Model Spec\n",
    "            print(\"\\n\\n----------MODEL SPEC----------\")\n",
    "            print(f\"Model Type: {model}\")\n",
    "            print(f\"Target Label: {label}\")\n",
    "            print(f\"Prediction Mode: {mode}\")\n",
    "\n",
    "            # Define Model\n",
    "            if model==\"LogisticRegression\":\n",
    "                # Define LogisticRegression Classifier + Paramgrid\n",
    "                model_instance = LogisticRegression(\n",
    "                                featuresCol=\"scaledFeatures\",\n",
    "                                labelCol=label,\n",
    "                                standardization=False,\n",
    "                                weightCol=label + \"_weighted\")\n",
    "                paramGrid = ParamGridBuilder().addGrid(model_instance.maxIter, [50,100])\\\n",
    "                                 .addGrid(model_instance.regParam, [0.001,0.01,0.1]) \\\n",
    "                                 .addGrid(model_instance.elasticNetParam, [0.0,0.5]) \\\n",
    "                                 .build()\n",
    "                params = [\"maxIter\",\"regParam\",\"elasticNetParam\"]\n",
    "            if model==\"SVM\":\n",
    "                # Define SVM Classifier + Paramgrid\n",
    "                model_instance = LinearSVC(labelCol=label,\n",
    "                                            featuresCol=\"scaledFeatures\",\n",
    "                                            standardization=False,\n",
    "                                            weightCol=label + \"_weighted\")\n",
    "                paramGrid = ParamGridBuilder().addGrid(model_instance.aggregationDepth, [2,3])\\\n",
    "                                 .addGrid(model_instance.maxIter, [100,300]) \\\n",
    "                                 .addGrid(model_instance.regParam, [0.01,0.001,0.0001]) \\\n",
    "                                 .build()\n",
    "                params = [\"aggregationDepth\",\"maxIter\",\"regParam\"]\n",
    "                \n",
    "\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=label)\n",
    "            cv = CrossValidator(estimator=model_instance, evaluator=evaluator, \\\n",
    "                          estimatorParamMaps=paramGrid, numFolds=5, parallelism=2)\n",
    "            cvModel = cv.fit(train)\n",
    "            model_best = cvModel.bestModel\n",
    "            param_print = '\\n'.join([line for line in model_best.explainParams().split('\\n') if line.split(\":\")[0] in params])\n",
    "            print(\"Chosen parameters: \\n\" + param_print)\n",
    "\n",
    "            print(str(model) + \" Coefficients: \" + str(dict(zip(features,[\"{:.4f}\".format(a) for a in model_best.coefficients]))))\n",
    "            print(str(model) + \" Intercept: \" + \"{:.4f}\".format(model_best.intercept))\n",
    "\n",
    "            # Predict and evaluate\n",
    "            predictions = cvModel.transform(test)\n",
    "            predictionAndLabels = predictions.select(predictions.prediction, label)\n",
    "\n",
    "            countcorrect = predictionAndLabels.filter(f\"{label} == prediction\").count()\n",
    "            countincorrect = predictionAndLabels.filter(f\"{label} != prediction\").count()\n",
    "            countall = predictionAndLabels.count()\n",
    "            accuracy = countcorrect/countall\n",
    "            print(f\"Count correct: {countcorrect}\")\n",
    "            print(f\"Count incorrect: {countincorrect}\")\n",
    "            print(f\"Count all: {countall}\")\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(f\"Test Error {1-accuracy}\")\n",
    "            \n",
    "            predictionAndLabels = predictions.select(\"prediction\", label).rdd.map(lambda p: [p[0], float(p[1])]) # Map to RDD prediction|label\n",
    "            metrics =  MulticlassMetrics(predictionAndLabels)\n",
    "            confusion = metrics.confusionMatrix()\n",
    "            print(\"Confusion matrix: \\n\" , confusion)\n",
    "            print(f\"Weighted F-Score: {metrics.weightedFMeasure()}\")\n",
    "            \n",
    "            \n",
    "            print(\"--------------------\")\n",
    "\n",
    "            newRow = spark.createDataFrame([(model,label,mode,param_print,accuracy,metrics.weightedFMeasure(),1-accuracy)], schema)\n",
    "            evalDF = evalDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a22d6e-f50b-48d5-87dc-667e1f078f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evalDF.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead53265-6305-4ed0-a9e9-d6a150663c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
