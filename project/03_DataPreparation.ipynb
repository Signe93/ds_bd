{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d40df1-33d4-4845-8100-7b15f96be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb235b29-0dc0-4efa-b572-63dbc88bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/Flight_Delay_Jan_2020_ontime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a794193-0fee-4f5b-ab7d-d04f71168efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"FlightDataStatistics\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81c97e-929d-49db-915a-b88d5cf4e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(inputFile) \\\n",
    "        .withColumn(\"DIVERTED_BOOL\", expr(\"DIVERTED\").cast(BooleanType())) \\\n",
    "        .withColumn(\"CANCELLED_BOOL\", expr(\"CANCELLED\").cast(BooleanType())) \\\n",
    "        .withColumn(\"DEP_DEL15_BOOL\", expr(\"DEP_DEL15\").cast(BooleanType())) \\\n",
    "        .withColumn(\"ARR_DEL15_BOOL\", expr(\"ARR_DEL15\").cast(BooleanType())) \\\n",
    "        \n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16690424-37a7-4583-9592-bf53bd56c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether OP_CARRIER_FL_NUM is merely running id for flights or rather encoding specific trims (e.g. Istanbul -> New York)\n",
    "pysparkDF.groupby('OP_CARRIER_FL_NUM').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917b10-342b-4032-b0ef-cde565dab64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# A tail number refers to an identification number painted on an aircraft, frequently on the tail.\n",
    "# Check amount of flights per plane\n",
    "pysparkDF.groupby('TAIL_NUM').count().show()\n",
    "\n",
    "# Check average flights per plane per year\n",
    "pysparkDF.groupby('TAIL_NUM').count().agg(F.mean('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feb0f3-b715-400e-8395-db318f6934ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether ORIGIN_AIRPORT_ID is 1:1 mapping onto ORIGIN\n",
    "pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN']).count().count() == pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN']).count().dropDuplicates(['ORIGIN_AIRPORT_ID']).count()\n",
    "\n",
    "# -> ORIGIN_AIRPORT_ID is string indexing ORIGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f080c9d-7c73-4262-b5a6-8908149778ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether ORIGIN_AIRPORT_ID is 1:1 mapping onto ORIGIN_AIRPORT_SEQ_ID\n",
    "pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID']).count().count() == pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID']).count().dropDuplicates(['ORIGIN_AIRPORT_ID']).count()\n",
    "\n",
    "# -> ORIGIN_AIRPORT_ID is 1:1 mapping to ORIGIN_AIRPORT_SEQ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee88c9b-ecc1-4fa0-b6a4-b72087f84563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether DEST_AIRPORT_ID is 1:1 mapping onto DEST\n",
    "pysparkDF.groupby(['DEST_AIRPORT_ID','DEST']).count().count() == pysparkDF.groupby(['DEST_AIRPORT_ID','DEST']).count().dropDuplicates(['DEST_AIRPORT_ID']).count()\n",
    "\n",
    "# -> DEST_AIRPORT_ID is string indexing DEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a09bc-a7a7-4701-ab66-dad738b6d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether DEST_AIRPORT_ID is 1:1 mapping onto DEST_AIRPORT_SEQ_ID\n",
    "pysparkDF.groupby(['DEST_AIRPORT_ID','DEST_AIRPORT_SEQ_ID']).count().count() == pysparkDF.groupby(['DEST_AIRPORT_ID','DEST_AIRPORT_SEQ_ID']).count().dropDuplicates(['DEST_AIRPORT_SEQ_ID']).count()\n",
    "\n",
    "# -> DEST_AIRPORT_ID is 1:1 mapping to ORIGIN_AIRPORT_SEQ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b0cd7-b51e-40cd-bdc9-3d6917254eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether OP_UNIQUE_CARRIER is 1:1 mapping onto OP_CARRIER\n",
    "pysparkDF.groupby(['OP_UNIQUE_CARRIER','OP_CARRIER']).count().count() == pysparkDF.groupby(['OP_UNIQUE_CARRIER','OP_CARRIER']).count().dropDuplicates(['OP_UNIQUE_CARRIER']).count()\n",
    "\n",
    "# -> OP_UNIQUE_CARRIER is 1:1 mapping to OP_CARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9613f2-a306-4f71-b8b7-48c3d9b67578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether TAIL_NUM is 1:1 mapping onto OP_CARRIER_FL_NUM\n",
    "pysparkDF.groupby(['TAIL_NUM','OP_CARRIER_FL_NUM']).count().count() == pysparkDF.groupby(['TAIL_NUM','OP_CARRIER_FL_NUM']).count().dropDuplicates(['OP_CARRIER_FL_NUM']).count()\n",
    "\n",
    "# -> TAIL_NUM is not 1:1 mapping to OP_CARRIER_FL_NUM -> One distinct plane can fly multiple routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb783a-d512-49ff-afb8-145b852ff68b",
   "metadata": {},
   "source": [
    "### Remove faulty features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd1f21-4dd2-443a-80ee-2d9a4be1cbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pysparkDF = pysparkDF.drop('_c21')\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d069aa-0c9c-4bec-87b6-fee149db57b1",
   "metadata": {},
   "source": [
    "### Remove records containing NULLvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1ce3-5ff6-4521-be22-2acafffd4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF_nonull = pysparkDF.dropna()\n",
    "f\"Removed {pysparkDF.count()-pysparkDF_nonull.count()} records containing NULL values\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4059e-49e5-452e-b592-a2b3465cab47",
   "metadata": {},
   "source": [
    "### Build String indexer for TAIL_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff308-39fc-490f-8416-e18fef441861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailNum_Indexer = StringIndexer().setInputCol(\"TAIL_NUM\").setOutputCol(\"TAIL_NUM_ID\").fit(pysparkDF_nonull)\n",
    "pysparkDF_indexed = tailNum_Indexer.transform(pysparkDF_nonull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8baf3-2876-4edb-ad65-718dc0c8fd7a",
   "metadata": {},
   "source": [
    "### Define label columns and remove non-BOOL label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcfd64-4fa0-4f76-b0cf-f61d71bfb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCols = [\"DEP_DEL15_BOOL\",\"ARR_DEL15_BOOL\",\"CANCELLED_BOOL\",\"DIVERTED_BOOL\"]\n",
    "\n",
    "pysparkDF_nonBOOL = pysparkDF_indexed\n",
    "for col in [label[:-5] for label in labelCols]:\n",
    "    pysparkDF_nonBOOL = pysparkDF_nonBOOL.drop(col)\n",
    "pysparkDF_nonBOOL.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea5e71-bb8c-44bf-b058-6b0bd155795b",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for unconditional prediction\n",
    "-> Unconditional is referring to predicting each of the labels without having information on the current status of the flight (Use-Case: Checking the day before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13657d-d85e-4833-b671-dfcacbd418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_unconditional = pysparkDF_nonBOOL.columns.copy()\n",
    "featureCols_unconditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_unconditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in labelCols:\n",
    "    featureCols_unconditional.remove(label)\n",
    "                                     \n",
    "featureCols_unconditional                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae89514-182f-4972-a212-e73435c11ab7",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for conditional prediction\n",
    "-> Conditional is referring to predicting each of the labels considering available real-time information on the current status of the flight (Use-Case: Checking while at the airport, pre-flight)\n",
    "\n",
    "One would expect that prediction performance is increased when the model is aware of the current flight status (=DEP_DEL15)\n",
    "\n",
    "Example: If the model is aware that the flight has departure delay, it might be able to better predict whether it will also be delayed at arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30055325-c3d0-43ed-890a-9ecc0e78d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_conditional = pysparkDF_nonBOOL.columns.copy()\n",
    "featureCols_conditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_conditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in [label for label in labelCols if label!=\"DEP_DEL15\"]:\n",
    "    featureCols_conditional.remove(label)\n",
    "    \n",
    "featureCols_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e42ee-540f-46e6-8058-a43e2127d260",
   "metadata": {},
   "source": [
    "### Build and apply feature column assembler for both featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7bff2-24d5-4d7e-a5e9-64cf2e753a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_unconditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_unconditional))\n",
    "assembler_conditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_conditional))\n",
    "\n",
    "featureSet_unconditional = assembler_unconditional.transform(pysparkDF_nonBOOL)\n",
    "featureSet_conditional = assembler_conditional.transform(pysparkDF_nonBOOL)\n",
    "\n",
    "# Define same base-scaler for both feature cols\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, \n",
    "                        withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel_unconditional = scaler.fit(featureSet_unconditional)\n",
    "scalerModel_conditional = scaler.fit(featureSet_conditional)\n",
    "\n",
    "scaledFeatureSet_unconditional = scalerModel_unconditional.transform(featureSet_unconditional)\n",
    "scaledFeatureSet_conditional = scalerModel_conditional.transform(featureSet_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a406d7a-4fdc-4e57-a2c1-27fb1d8bc5f7",
   "metadata": {},
   "source": [
    "### Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3687a6b-fbb2-4250-8282-4aa6cd61f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_unconditional = scaledFeatureSet_unconditional.randomSplit([0.9, 0.1 ], 12345)\n",
    "training_unconditional = splits_unconditional[0]\n",
    "test_unconditional = splits_unconditional[1]\n",
    "\n",
    "splits_conditional= scaledFeatureSet_conditional.randomSplit([0.9, 0.1 ], 12345)\n",
    "training_conditional = splits_conditional[0]\n",
    "test_conditional = splits_conditional[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead53265-6305-4ed0-a9e9-d6a150663c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
