{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d40df1-33d4-4845-8100-7b15f96be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb235b29-0dc0-4efa-b572-63dbc88bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/Flight_Delay_Jan_2020_ontime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a794193-0fee-4f5b-ab7d-d04f71168efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"FlightDataStatistics\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81c97e-929d-49db-915a-b88d5cf4e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(inputFile)\n",
    "        \n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb783a-d512-49ff-afb8-145b852ff68b",
   "metadata": {},
   "source": [
    "### Remove faulty features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd1f21-4dd2-443a-80ee-2d9a4be1cbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pysparkDF = pysparkDF.drop('_c21')\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d069aa-0c9c-4bec-87b6-fee149db57b1",
   "metadata": {},
   "source": [
    "### Remove records containing NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1ce3-5ff6-4521-be22-2acafffd4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF_nonull = pysparkDF.dropna()\n",
    "count_pysparkDF = pysparkDF.count()\n",
    "count_pysparkDF_nonull = pysparkDF_nonull.count()\n",
    "\n",
    "print(\"Count DF records: \" + str(count_pysparkDF))\n",
    "print(\"Count DF records with removed NULL values: \" + str(count_pysparkDF_nonull))\n",
    "\n",
    "f\"Removed {count_pysparkDF-count_pysparkDF_nonull} records containing NULL values\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4059e-49e5-452e-b592-a2b3465cab47",
   "metadata": {},
   "source": [
    "### Build String indexer for TAIL_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff308-39fc-490f-8416-e18fef441861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailNum_Indexer = StringIndexer().setInputCol(\"TAIL_NUM\").setOutputCol(\"TAIL_NUM_ID\").fit(pysparkDF_nonull)\n",
    "pysparkDF_indexed = tailNum_Indexer.transform(pysparkDF_nonull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8baf3-2876-4edb-ad65-718dc0c8fd7a",
   "metadata": {},
   "source": [
    "### Define label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcfd64-4fa0-4f76-b0cf-f61d71bfb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCols = [\"DEP_DEL15\",\"ARR_DEL15\",\"CANCELLED\",\"DIVERTED\"]\n",
    "labelCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea5e71-bb8c-44bf-b058-6b0bd155795b",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for unconditional prediction\n",
    "-> Unconditional is referring to predicting each of the labels without having information on the current status of the flight (Use-Case: Checking the day before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13657d-d85e-4833-b671-dfcacbd418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. \n",
    "#         Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_unconditional = pysparkDF_indexed.columns.copy()\n",
    "featureCols_unconditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_unconditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEST\") # -> DEST_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in labelCols:\n",
    "    featureCols_unconditional.remove(label)\n",
    "                                     \n",
    "featureCols_unconditional                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae89514-182f-4972-a212-e73435c11ab7",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for conditional prediction\n",
    "-> Conditional is referring to predicting each of the labels considering available real-time information on the current status of the flight (Use-Case: Checking while at the airport, pre-flight)\n",
    "\n",
    "One would expect that prediction performance is increased when the model is aware of the current flight status (=DEP_DEL15)\n",
    "\n",
    "Example: If the model is aware that the flight has departure delay, it might be able to better predict whether it will also be delayed at arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30055325-c3d0-43ed-890a-9ecc0e78d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. \n",
    "#         Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_conditional = pysparkDF_indexed.columns.copy()\n",
    "featureCols_conditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_conditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEST\") # -> DEST_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in [label for label in labelCols if label!=\"DEP_DEL15\"]:\n",
    "    featureCols_conditional.remove(label)\n",
    "    \n",
    "featureCols_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e42ee-540f-46e6-8058-a43e2127d260",
   "metadata": {},
   "source": [
    "### Build and apply feature column assembler for both featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7bff2-24d5-4d7e-a5e9-64cf2e753a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_unconditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_unconditional))\n",
    "assembler_conditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_conditional))\n",
    "\n",
    "featureSet_unconditional = assembler_unconditional.transform(pysparkDF_indexed)\n",
    "featureSet_conditional = assembler_conditional.transform(pysparkDF_indexed)\n",
    "\n",
    "# Define same base-scaler for both feature cols\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, \n",
    "                        withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel_unconditional = scaler.fit(featureSet_unconditional)\n",
    "scalerModel_conditional = scaler.fit(featureSet_conditional)\n",
    "\n",
    "scaledFeatureSet_unconditional = scalerModel_unconditional.transform(featureSet_unconditional)\n",
    "scaledFeatureSet_conditional = scalerModel_conditional.transform(featureSet_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a406d7a-4fdc-4e57-a2c1-27fb1d8bc5f7",
   "metadata": {},
   "source": [
    "### Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3687a6b-fbb2-4250-8282-4aa6cd61f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_unconditional = scaledFeatureSet_unconditional.randomSplit([0.8, 0.2], 12345)\n",
    "training_unconditional = splits_unconditional[0]\n",
    "test_unconditional = splits_unconditional[1]\n",
    "\n",
    "splits_conditional= scaledFeatureSet_conditional.randomSplit([0.8, 0.2], 12345)\n",
    "training_conditional = splits_conditional[0]\n",
    "test_conditional = splits_conditional[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead53265-6305-4ed0-a9e9-d6a150663c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
