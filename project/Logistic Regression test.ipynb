{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76d40df1-33d4-4845-8100-7b15f96be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cb235b29-0dc0-4efa-b572-63dbc88bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/Flight_Delay_Jan_2020_ontime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a794193-0fee-4f5b-ab7d-d04f71168efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"FlightDelay\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c81c97e-929d-49db-915a-b88d5cf4e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DEL15: double (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_DEL15: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pysparkDF = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(inputFile)\n",
    "\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb783a-d512-49ff-afb8-145b852ff68b",
   "metadata": {},
   "source": [
    "### Remove faulty features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1fd1f21-4dd2-443a-80ee-2d9a4be1cbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DEL15: double (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_DEL15: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pysparkDF = pysparkDF.drop('_c21')\n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d069aa-0c9c-4bec-87b6-fee149db57b1",
   "metadata": {},
   "source": [
    "### Remove records containing NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "336a1ce3-5ff6-4521-be22-2acafffd4909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 8078 records containing NULL values'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysparkDF_nonull = pysparkDF.dropna()\n",
    "f\"Removed {pysparkDF.count()-pysparkDF_nonull.count()} records containing NULL values\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4059e-49e5-452e-b592-a2b3465cab47",
   "metadata": {},
   "source": [
    "### Build String indexer for TAIL_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e20ff308-39fc-490f-8416-e18fef441861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailNum_Indexer = StringIndexer().setInputCol(\"TAIL_NUM\").setOutputCol(\"TAIL_NUM_ID\").fit(pysparkDF_nonull)\n",
    "pysparkDF_indexed = tailNum_Indexer.transform(pysparkDF_nonull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8baf3-2876-4edb-ad65-718dc0c8fd7a",
   "metadata": {},
   "source": [
    "### Define label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6fcfd64-4fa0-4f76-b0cf-f61d71bfb68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEP_DEL15', 'ARR_DEL15']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCols = [\"DEP_DEL15\",\"ARR_DEL15\"]\n",
    "labelCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea5e71-bb8c-44bf-b058-6b0bd155795b",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for unconditional prediction\n",
    "-> Unconditional is referring to predicting each of the labels without having information on the current status of the flight (Use-Case: Checking the day before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a13657d-d85e-4833-b671-dfcacbd418ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DAY_OF_MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'OP_CARRIER_AIRLINE_ID',\n",
       " 'OP_CARRIER_FL_NUM',\n",
       " 'ORIGIN_AIRPORT_ID',\n",
       " 'DEST_AIRPORT_ID',\n",
       " 'DEP_TIME',\n",
       " 'ARR_TIME',\n",
       " 'CANCELLED',\n",
       " 'DIVERTED',\n",
       " 'DISTANCE',\n",
       " 'TAIL_NUM_ID']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_unconditional = pysparkDF_indexed.columns.copy()\n",
    "featureCols_unconditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_unconditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_unconditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_unconditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in labelCols:\n",
    "    featureCols_unconditional.remove(label)\n",
    "                                     \n",
    "featureCols_unconditional                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24b73db9-1b59-4a10-bd5a-3595b2051df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols_unconditional.remove(\"CANCELLED\")\n",
    "featureCols_unconditional.remove(\"DIVERTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae89514-182f-4972-a212-e73435c11ab7",
   "metadata": {},
   "source": [
    "### Remove redundant features and labels for conditional prediction\n",
    "-> Conditional is referring to predicting each of the labels considering available real-time information on the current status of the flight (Use-Case: Checking while at the airport, pre-flight)\n",
    "\n",
    "One would expect that prediction performance is increased when the model is aware of the current flight status (=DEP_DEL15)\n",
    "\n",
    "Example: If the model is aware that the flight has departure delay, it might be able to better predict whether it will also be delayed at arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30055325-c3d0-43ed-890a-9ecc0e78d50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DAY_OF_MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'OP_CARRIER_AIRLINE_ID',\n",
       " 'OP_CARRIER_FL_NUM',\n",
       " 'ORIGIN_AIRPORT_ID',\n",
       " 'DEST_AIRPORT_ID',\n",
       " 'DEP_TIME',\n",
       " 'DEP_DEL15',\n",
       " 'ARR_TIME',\n",
       " 'CANCELLED',\n",
       " 'DIVERTED',\n",
       " 'DISTANCE',\n",
       " 'TAIL_NUM_ID']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove strings from id/string pairs (redundant)\n",
    "# Remark: since in this dataset both string and id exist already, no further preprocessing via string_indexer is necessary. Otherwise, strings would have first been converted to ids via string_indexer.\n",
    "featureCols_conditional = pysparkDF_indexed.columns.copy()\n",
    "featureCols_conditional.remove(\"TAIL_NUM\") # -> TAIL_NUM_ID\n",
    "featureCols_conditional.remove(\"OP_UNIQUE_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"OP_CARRIER\") # -> OP_CARRIER_AIRLINE_ID\n",
    "featureCols_conditional.remove(\"ORIGIN\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"ORIGIN_AIRPORT_SEQ_ID\") # -> ORIGIN_AIRPORT_ID\n",
    "featureCols_conditional.remove(\"DEST\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEST_AIRPORT_SEQ_ID\") # -> DEST_AIRPORT_SEQ_ID\n",
    "featureCols_conditional.remove(\"DEP_TIME_BLK\") # -> preliminary elimination, check if model works better with binned values or not\n",
    "\n",
    "for label in [label for label in labelCols if label!=\"DEP_DEL15\"]:\n",
    "    featureCols_conditional.remove(label)\n",
    "    \n",
    "featureCols_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e42ee-540f-46e6-8058-a43e2127d260",
   "metadata": {},
   "source": [
    "### Build and apply feature column assembler for both featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eea7bff2-24d5-4d7e-a5e9-64cf2e753a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_unconditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_unconditional))\n",
    "assembler_conditional =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols_conditional))\n",
    "\n",
    "featureSet_unconditional = assembler_unconditional.transform(pysparkDF_indexed)\n",
    "featureSet_conditional = assembler_conditional.transform(pysparkDF_indexed)\n",
    "\n",
    "# Define same base-scaler for both feature cols\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, \n",
    "                        withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel_unconditional = scaler.fit(featureSet_unconditional)\n",
    "scalerModel_conditional = scaler.fit(featureSet_conditional)\n",
    "\n",
    "scaledFeatureSet_unconditional = scalerModel_unconditional.transform(featureSet_unconditional)\n",
    "scaledFeatureSet_conditional = scalerModel_conditional.transform(featureSet_conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ce5e058f-508f-4518-93b8-a0eeb93d4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRatio(df):\n",
    "    delayedDf = df.filter(\"DEP_DEL15=1.0\")\n",
    "    sampleRatio = delayedDf.count() / df.count()\n",
    "    return sampleRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9a801dd0-e3fb-4b01-aa58-8ce1ee793083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8643811376033455\n",
      "0.13561886239665455\n"
     ]
    }
   ],
   "source": [
    "ratioOfDelayed = getRatio(training_unconditional)\n",
    "delayedWeight  = 1 - ratioOfDelayed\n",
    "nonDelayedWeight = ratioOfDelayed\n",
    "print(delayedWeight)\n",
    "print(nonDelayedWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95cc3a9c-e85b-4ef0-9d1d-bb200934ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DAY_OF_MONTH: int, DAY_OF_WEEK: int, OP_UNIQUE_CARRIER: string, OP_CARRIER_AIRLINE_ID: int, OP_CARRIER: string, TAIL_NUM: string, OP_CARRIER_FL_NUM: int, ORIGIN_AIRPORT_ID: int, ORIGIN_AIRPORT_SEQ_ID: int, ORIGIN: string, DEST_AIRPORT_ID: int, DEST_AIRPORT_SEQ_ID: int, DEST: string, DEP_TIME: int, DEP_DEL15: double, DEP_TIME_BLK: string, ARR_TIME: int, ARR_DEL15: double, CANCELLED: double, DIVERTED: double, DISTANCE: double, TAIL_NUM_ID: double, features: vector]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureSet_unconditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "052cc1dd-667d-498b-9301-bcf593879a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DAY_OF_MONTH: int, DAY_OF_WEEK: int, OP_UNIQUE_CARRIER: string, OP_CARRIER_AIRLINE_ID: int, OP_CARRIER: string, TAIL_NUM: string, OP_CARRIER_FL_NUM: int, ORIGIN_AIRPORT_ID: int, ORIGIN_AIRPORT_SEQ_ID: int, ORIGIN: string, DEST_AIRPORT_ID: int, DEST_AIRPORT_SEQ_ID: int, DEST: string, DEP_TIME: int, DEP_DEL15: double, DEP_TIME_BLK: string, ARR_TIME: int, ARR_DEL15: double, CANCELLED: double, DIVERTED: double, DISTANCE: double, TAIL_NUM_ID: double, features: vector, scaledFeatures: vector, DEP_DEL15_weighted: double]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightedDF = scaledFeatureSet_unconditional.withColumn(\"DEP_DEL15_weighted\", F.when(scaledFeatureSet_unconditional[\"DEP_DEL15\"]==(\"1.0\"),delayedWeight).otherwise(nonDelayedWeight))\n",
    "weightedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a406d7a-4fdc-4e57-a2c1-27fb1d8bc5f7",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### Split data into training and test set\n",
    "Die Aufteilung der Daten erfolgt in 80% Trainingsdaten und 20% Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3687a6b-fbb2-4250-8282-4aa6cd61f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_unconditional = weightedDF.randomSplit([0.8, 0.2], 12345)\n",
    "training_unconditional = splits_unconditional[0]\n",
    "test_unconditional = splits_unconditional[1]\n",
    "\n",
    "splits_conditional= scaledFeatureSet_conditional.randomSplit([0.8, 0.2], 12345)\n",
    "training_conditional = splits_conditional[0]\n",
    "test_conditional = splits_conditional[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9179570c-fca3-4d10-b966-89d7f52ecc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: integer (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DEL15: double (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_DEL15: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- TAIL_NUM_ID: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- DEP_DEL15_weighted: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_unconditional.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b49f5-d45a-43f3-b37a-3c0a11e6db85",
   "metadata": {},
   "source": [
    "### Build and train the Logistic Regression Model\n",
    "\n",
    "Um eine Logistische Regression durchführen zu können, muss zunächst das zugehörige Element `LogisticRegression` aus der Bibliothek `pyspark.ml.classification` importiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0262f17-ba05-41cb-b7e4-a991cdc409ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f1f4084-6ea4-4fc6-824b-3464710d55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count correct: 103594\n",
      "count incorrect: 16456\n",
      "count all: 120050\n",
      "accuracy: 0.862923781757601\n",
      "Test Error 0.13707621824239902\n",
      "Confusion matrix: \n",
      " DenseMatrix([[103423.,      0.],\n",
      "             [ 16627.,      0.]])\n"
     ]
    }
   ],
   "source": [
    "model_instance = LogisticRegression(\n",
    "               featuresCol=\"scaledFeatures\",\n",
    "               labelCol=\"DEP_DEL15\",\n",
    "               standardization=False)\n",
    "paramGrid = ParamGridBuilder().addGrid(model_instance.maxIter, [20])\\\n",
    "                 .addGrid(model_instance.regParam, [0.7]) \\\n",
    "                 .addGrid(model_instance.elasticNetParam, [1.0]) \\\n",
    "                 .build()\n",
    "params = [\"maxIter\",\"regParam\",\"elasticNetParam\"]\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"DEP_DEL15\")\n",
    "cv = CrossValidator(estimator=model_instance, evaluator=evaluator, \\\n",
    "              estimatorParamMaps=paramGrid, numFolds=5, parallelism=2)\n",
    "cvModel = cv.fit(training_unconditional)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = cvModel.transform(test_unconditional)\n",
    "predictionAndLabels = predictions.select(predictions.prediction, \"DEP_DEL15\")\n",
    "\n",
    "countcorrect = predictionAndLabels.filter(f\"DEP_DEL15 == prediction\").count()\n",
    "countincorrect = predictionAndLabels.filter(f\"DEP_DEL15 != prediction\").count()\n",
    "countall = predictionAndLabels.count()\n",
    "accuracy = countcorrect/countall\n",
    "print(f\"count correct: {countcorrect}\")\n",
    "print(f\"count incorrect: {countincorrect}\")\n",
    "print(f\"count all: {countall}\")\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"Test Error {1-accuracy}\")\n",
    "\n",
    "predictionAndLabels = predictions.select(\"prediction\", label).rdd.map(lambda p: [p[0], float(p[1])]) # Map to RDD prediction|label\n",
    "metrics =  MulticlassMetrics(predictionAndLabels)\n",
    "confusion = metrics.confusionMatrix()            \n",
    "print(\"Confusion matrix: \\n\" , confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "52d84924-24fa-41f2-8506-416552f8c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N131EV', OP_CARRIER_FL_NUM=4867, ORIGIN_AIRPORT_ID=11423, ORIGIN_AIRPORT_SEQ_ID=1142307, ORIGIN='DSM', DEST_AIRPORT_ID=13487, DEST_AIRPORT_SEQ_ID=1348702, DEST='MSP', DEP_TIME=1347, DEP_DEL15=1.0, DEP_TIME_BLK='1300-1359', ARR_TIME=1445, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=232.0, TAIL_NUM_ID=1192.0, features=DenseVector([1.0, 3.0, 20363.0, 4867.0, 11423.0, 13487.0, 1347.0, 1445.0, 232.0, 1192.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.672, 7.4914, 8.8442, 2.71, 2.7502, 0.3943, 0.8524]), DEP_DEL15_weighted=0.8643811376033455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N131EV', OP_CARRIER_FL_NUM=5049, ORIGIN_AIRPORT_ID=13487, ORIGIN_AIRPORT_SEQ_ID=1348702, ORIGIN='MSP', DEST_AIRPORT_ID=11423, DEST_AIRPORT_SEQ_ID=1142307, DEST='DSM', DEP_TIME=1215, DEP_DEL15=1.0, DEP_TIME_BLK='1100-1159', ARR_TIME=1319, ARR_DEL15=1.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=232.0, TAIL_NUM_ID=1192.0, features=DenseVector([1.0, 3.0, 20363.0, 5049.0, 13487.0, 11423.0, 1215.0, 1319.0, 232.0, 1192.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.7719, 8.8451, 7.4907, 2.4444, 2.5104, 0.3943, 0.8524]), DEP_DEL15_weighted=0.8643811376033455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N131EV', OP_CARRIER_FL_NUM=5209, ORIGIN_AIRPORT_ID=13487, ORIGIN_AIRPORT_SEQ_ID=1348702, ORIGIN='MSP', DEST_AIRPORT_ID=11193, DEST_AIRPORT_SEQ_ID=1119302, DEST='CVG', DEP_TIME=1540, DEP_DEL15=0.0, DEP_TIME_BLK='1500-1559', ARR_TIME=1810, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=596.0, TAIL_NUM_ID=1192.0, features=DenseVector([1.0, 3.0, 20363.0, 5209.0, 13487.0, 11193.0, 1540.0, 1810.0, 596.0, 1192.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.8597, 8.8451, 7.3399, 3.0983, 3.4449, 1.013, 0.8524]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N132EV', OP_CARRIER_FL_NUM=4762, ORIGIN_AIRPORT_ID=12953, ORIGIN_AIRPORT_SEQ_ID=1295304, ORIGIN='LGA', DEST_AIRPORT_ID=13931, DEST_AIRPORT_SEQ_ID=1393102, DEST='ORF', DEP_TIME=1556, DEP_DEL15=0.0, DEP_TIME_BLK='1500-1559', ARR_TIME=1702, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=296.0, TAIL_NUM_ID=1193.0, features=DenseVector([1.0, 3.0, 20363.0, 4762.0, 12953.0, 13931.0, 1556.0, 1702.0, 296.0, 1193.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.6143, 8.4949, 9.1353, 3.1305, 3.2393, 0.5031, 0.8531]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N133EV', OP_CARRIER_FL_NUM=4901, ORIGIN_AIRPORT_ID=11433, ORIGIN_AIRPORT_SEQ_ID=1143302, ORIGIN='DTW', DEST_AIRPORT_ID=11193, DEST_AIRPORT_SEQ_ID=1119302, DEST='CVG', DEP_TIME=1221, DEP_DEL15=0.0, DEP_TIME_BLK='1200-1259', ARR_TIME=1323, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=229.0, TAIL_NUM_ID=1194.0, features=DenseVector([1.0, 3.0, 20363.0, 4901.0, 11433.0, 11193.0, 1221.0, 1323.0, 229.0, 1194.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.6906, 7.498, 7.3399, 2.4565, 2.518, 0.3892, 0.8538]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N133EV', OP_CARRIER_FL_NUM=5093, ORIGIN_AIRPORT_ID=11193, ORIGIN_AIRPORT_SEQ_ID=1119302, ORIGIN='CVG', DEST_AIRPORT_ID=11433, DEST_AIRPORT_SEQ_ID=1143302, DEST='DTW', DEP_TIME=1722, DEP_DEL15=0.0, DEP_TIME_BLK='1700-1759', ARR_TIME=1829, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=229.0, TAIL_NUM_ID=1194.0, features=DenseVector([1.0, 3.0, 20363.0, 5093.0, 11193.0, 11433.0, 1722.0, 1829.0, 229.0, 1194.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.796, 7.3406, 7.4972, 3.4644, 3.481, 0.3892, 0.8538]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N133EV', OP_CARRIER_FL_NUM=5423, ORIGIN_AIRPORT_ID=11433, ORIGIN_AIRPORT_SEQ_ID=1143302, ORIGIN='DTW', DEST_AIRPORT_ID=14576, DEST_AIRPORT_SEQ_ID=1457606, DEST='ROC', DEP_TIME=2011, DEP_DEL15=0.0, DEP_TIME_BLK='2000-2059', ARR_TIME=2119, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=296.0, TAIL_NUM_ID=1194.0, features=DenseVector([1.0, 3.0, 20363.0, 5423.0, 11433.0, 14576.0, 2011.0, 2119.0, 296.0, 1194.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.9772, 7.498, 9.5583, 4.0458, 4.033, 0.5031, 0.8538]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N134EV', OP_CARRIER_FL_NUM=4752, ORIGIN_AIRPORT_ID=13795, ORIGIN_AIRPORT_SEQ_ID=1379502, ORIGIN='OAJ', DEST_AIRPORT_ID=10397, DEST_AIRPORT_SEQ_ID=1039707, DEST='ATL', DEP_TIME=618, DEP_DEL15=0.0, DEP_TIME_BLK='0600-0659', ARR_TIME=758, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=399.0, TAIL_NUM_ID=912.0, features=DenseVector([1.0, 3.0, 20363.0, 4752.0, 13795.0, 10397.0, 618.0, 758.0, 399.0, 912.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.6088, 9.0471, 6.8179, 1.2433, 1.4427, 0.6782, 0.6522]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N134EV', OP_CARRIER_FL_NUM=4811, ORIGIN_AIRPORT_ID=10397, ORIGIN_AIRPORT_SEQ_ID=1039707, ORIGIN='ATL', DEST_AIRPORT_ID=10599, DEST_AIRPORT_SEQ_ID=1059904, DEST='BHM', DEP_TIME=847, DEP_DEL15=0.0, DEP_TIME_BLK='0800-0859', ARR_TIME=833, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=134.0, TAIL_NUM_ID=912.0, features=DenseVector([1.0, 3.0, 20363.0, 4811.0, 10397.0, 10599.0, 847.0, 833.0, 134.0, 912.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.6412, 6.8186, 6.9503, 1.704, 1.5854, 0.2278, 0.6522]), DEP_DEL15_weighted=0.13561886239665455),\n",
       " Row(DAY_OF_MONTH=1, DAY_OF_WEEK=3, OP_UNIQUE_CARRIER='9E', OP_CARRIER_AIRLINE_ID=20363, OP_CARRIER='9E', TAIL_NUM='N134EV', OP_CARRIER_FL_NUM=4811, ORIGIN_AIRPORT_ID=10599, ORIGIN_AIRPORT_SEQ_ID=1059904, ORIGIN='BHM', DEST_AIRPORT_ID=10397, DEST_AIRPORT_SEQ_ID=1039707, DEST='ATL', DEP_TIME=921, DEP_DEL15=0.0, DEP_TIME_BLK='0900-0959', ARR_TIME=1109, ARR_DEL15=0.0, CANCELLED=0.0, DIVERTED=0.0, DISTANCE=134.0, TAIL_NUM_ID=912.0, features=DenseVector([1.0, 3.0, 20363.0, 4811.0, 10599.0, 10397.0, 921.0, 1109.0, 134.0, 912.0]), scaledFeatures=DenseVector([0.1109, 1.569, 54.271, 2.6412, 6.9511, 6.8179, 1.8529, 2.1107, 0.2278, 0.6522]), DEP_DEL15_weighted=0.13561886239665455)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_unconditional.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "383e9f3f-fe1d-4602-b815-e98d64c12046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count correct: 71268\n",
      "count incorrect: 48782\n",
      "count all: 120050\n",
      "accuracy: 0.5936526447313619\n",
      "Test Error 0.4063473552686381\n",
      "Confusion matrix: \n",
      " DenseMatrix([[60583., 42840.],\n",
      "             [ 6883.,  9744.]])\n"
     ]
    }
   ],
   "source": [
    "model_instance = LogisticRegression(\n",
    "               featuresCol=\"scaledFeatures\",\n",
    "               labelCol=\"DEP_DEL15\",\n",
    "               weightCol=\"DEP_DEL15_weighted\")\n",
    "paramGrid = ParamGridBuilder().addGrid(model_instance.maxIter, [50])\\\n",
    "                 .addGrid(model_instance.regParam, [0.1]) \\\n",
    "                 .addGrid(model_instance.elasticNetParam, [0.0]) \\\n",
    "                 .build()\n",
    "params = [\"maxIter\",\"regParam\",\"elasticNetParam\"]\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"DEP_DEL15\")\n",
    "cv = CrossValidator(estimator=model_instance, evaluator=evaluator, \\\n",
    "              estimatorParamMaps=paramGrid, numFolds=5, parallelism=2)\n",
    "cvModel = cv.fit(training_unconditional)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = cvModel.transform(test_unconditional)\n",
    "predictionAndLabels = predictions.select(predictions.prediction, \"DEP_DEL15\")\n",
    "\n",
    "countcorrect = predictionAndLabels.filter(f\"DEP_DEL15 == prediction\").count()\n",
    "countincorrect = predictionAndLabels.filter(f\"DEP_DEL15 != prediction\").count()\n",
    "countall = predictionAndLabels.count()\n",
    "accuracy = countcorrect/countall\n",
    "print(f\"count correct: {countcorrect}\")\n",
    "print(f\"count incorrect: {countincorrect}\")\n",
    "print(f\"count all: {countall}\")\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"Test Error {1-accuracy}\")\n",
    "\n",
    "predictionAndLabels = predictions.select(\"prediction\", label).rdd.map(lambda p: [p[0], float(p[1])]) # Map to RDD prediction|label\n",
    "metrics =  MulticlassMetrics(predictionAndLabels)\n",
    "confusion = metrics.confusionMatrix()            \n",
    "print(\"Confusion matrix: \\n\" , confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "32c6fdf5-fc11-4871-87a4-e2f5f22eba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----+------------+--------+----------+\n",
      "|model|target label|mode|param_config|accuracy|test error|\n",
      "+-----+------------+----+------------+--------+----------+\n",
      "+-----+------------+----+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "      StructField('model', StringType(), True),\n",
    "      StructField('target label', StringType(), True),\n",
    "      StructField('mode', StringType(), True),\n",
    "      StructField('param_config', StringType(), True),\n",
    "      StructField('accuracy', FloatType(), True),\n",
    "      StructField('test error', FloatType(), True)\n",
    "  ])\n",
    "\n",
    "evalDF = spark.createDataFrame([], schema)\n",
    "evalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2b8fedb-25a4-4767-bcc1-bbfb5de58123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------MODEL SPEC----------\n",
      "Model Type: LogisticRegression\n",
      "Target Label: DEP_DEL15\n",
      "Prediction Mode: Unconditional\n",
      "Chosen parameters: \n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)\n",
      "LogisticRegression Coefficients: {'DAY_OF_MONTH': '0.0000', 'DAY_OF_WEEK': '0.0000', 'OP_CARRIER_AIRLINE_ID': '0.0000', 'OP_CARRIER_FL_NUM': '0.0000', 'ORIGIN_AIRPORT_ID': '0.0000', 'DEST_AIRPORT_ID': '0.0000', 'DEP_TIME': '0.0000', 'ARR_TIME': '0.0000', 'DISTANCE': '0.0000', 'TAIL_NUM_ID': '0.0000'}\n",
      "LogisticRegression Intercept: -1.8522\n",
      "count correct: 103594\n",
      "count incorrect: 16456\n",
      "count all: 120050\n",
      "accuracy: 0.862923781757601\n",
      "Test Error 0.13707621824239902\n",
      "Confusion matrix: \n",
      " DenseMatrix([[103594.,      0.],\n",
      "             [ 16456.,      0.]])\n",
      "--------------------\n",
      "\n",
      "\n",
      "----------MODEL SPEC----------\n",
      "Model Type: LogisticRegression\n",
      "Target Label: ARR_DEL15\n",
      "Prediction Mode: Unconditional\n",
      "Chosen parameters: \n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)\n",
      "LogisticRegression Coefficients: {'DAY_OF_MONTH': '0.0000', 'DAY_OF_WEEK': '0.0000', 'OP_CARRIER_AIRLINE_ID': '0.0000', 'OP_CARRIER_FL_NUM': '0.0000', 'ORIGIN_AIRPORT_ID': '0.0000', 'DEST_AIRPORT_ID': '0.0000', 'DEP_TIME': '0.0000', 'ARR_TIME': '0.0000', 'DISTANCE': '0.0000', 'TAIL_NUM_ID': '0.0000'}\n",
      "LogisticRegression Intercept: -1.8403\n",
      "count correct: 103423\n",
      "count incorrect: 16627\n",
      "count all: 120050\n",
      "accuracy: 0.8614993752603082\n",
      "Test Error 0.13850062473969182\n",
      "Confusion matrix: \n",
      " DenseMatrix([[103423.,      0.],\n",
      "             [ 16627.,      0.]])\n",
      "--------------------\n",
      "\n",
      "\n",
      "----------MODEL SPEC----------\n",
      "Model Type: LogisticRegression\n",
      "Target Label: ARR_DEL15\n",
      "Prediction Mode: Conditional\n",
      "Chosen parameters: \n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)\n",
      "LogisticRegression Coefficients: {'DAY_OF_MONTH': '0.0000', 'DAY_OF_WEEK': '0.0000', 'OP_CARRIER_AIRLINE_ID': '0.0000', 'OP_CARRIER_FL_NUM': '0.0000', 'ORIGIN_AIRPORT_ID': '0.0000', 'DEST_AIRPORT_ID': '0.0000', 'DEP_TIME': '0.0000', 'DEP_DEL15': '0.0000', 'ARR_TIME': '0.0000', 'CANCELLED': '0.0000', 'DIVERTED': '0.0000', 'DISTANCE': '0.0000', 'TAIL_NUM_ID': '0.0000'}\n",
      "LogisticRegression Intercept: -1.8403\n",
      "count correct: 103423\n",
      "count incorrect: 16627\n",
      "count all: 120050\n",
      "accuracy: 0.8614993752603082\n",
      "Test Error 0.13850062473969182\n",
      "Confusion matrix: \n",
      " DenseMatrix([[103423.,      0.],\n",
      "             [ 16627.,      0.]])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "models = [\"LogisticRegression\"]\n",
    "\n",
    "for model in models:\n",
    "    for label in labelCols:\n",
    "        for test, train, mode, features in zip([test_unconditional, test_conditional],[training_unconditional,training_conditional],[\"Unconditional\",\"Conditional\"],[featureCols_unconditional,featureCols_conditional]):\n",
    "            # Skip invalid combinations\n",
    "            if label==\"DEP_DEL15\" and mode==\"Conditional\":\n",
    "                continue\n",
    "\n",
    "            # Print Model Spec\n",
    "            print(\"\\n\\n----------MODEL SPEC----------\")\n",
    "            print(f\"Model Type: {model}\")\n",
    "            print(f\"Target Label: {label}\")\n",
    "            print(f\"Prediction Mode: {mode}\")\n",
    "\n",
    "            # Define Model\n",
    "            if model==\"LogisticRegression\":\n",
    "                # Define LogisticRegression Classifier acc. to current param\n",
    "                model_instance = LogisticRegression(\n",
    "                               featuresCol=\"scaledFeatures\",\n",
    "                               labelCol=label,\n",
    "                               standardization=False)\n",
    "                paramGrid = ParamGridBuilder().addGrid(model_instance.maxIter, [20])\\\n",
    "                                 .addGrid(model_instance.regParam, [0.7]) \\\n",
    "                                 .addGrid(model_instance.elasticNetParam, [1.0]) \\\n",
    "                                 .build()\n",
    "                params = [\"maxIter\",\"regParam\",\"elasticNetParam\"]\n",
    "            if model==\"SVM\":\n",
    "                # Define SVM Classifier acc. to current param\n",
    "                pass\n",
    "\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=label)\n",
    "            cv = CrossValidator(estimator=model_instance, evaluator=evaluator, \\\n",
    "                          estimatorParamMaps=paramGrid, numFolds=5, parallelism=2)\n",
    "            cvModel = cv.fit(train)\n",
    "            model_best = cvModel.bestModel\n",
    "            param_print = '\\n'.join([line for line in model_best.explainParams().split('\\n') if line.split(\":\")[0] in params])\n",
    "            print(\"Chosen parameters: \\n\" + param_print)\n",
    "\n",
    "            print(str(model) + \" Coefficients: \" + str(dict(zip(features,[\"{:.4f}\".format(a) for a in model_best.coefficients]))))\n",
    "            print(str(model) + \" Intercept: \" + \"{:.4f}\".format(model_best.intercept))\n",
    "\n",
    "            # Predict and evaluate\n",
    "            predictions = cvModel.transform(test)\n",
    "            predictionAndLabels = predictions.select(predictions.prediction, label)\n",
    "\n",
    "            countcorrect = predictionAndLabels.filter(f\"{label} == prediction\").count()\n",
    "            countincorrect = predictionAndLabels.filter(f\"{label} != prediction\").count()\n",
    "            countall = predictionAndLabels.count()\n",
    "            accuracy = countcorrect/countall\n",
    "            print(f\"count correct: {countcorrect}\")\n",
    "            print(f\"count incorrect: {countincorrect}\")\n",
    "            print(f\"count all: {countall}\")\n",
    "            print(f\"accuracy: {accuracy}\")\n",
    "            print(f\"Test Error {1-accuracy}\")\n",
    "            \n",
    "            predictionAndLabels = predictions.select(\"prediction\", label).rdd.map(lambda p: [p[0], float(p[1])]) # Map to RDD prediction|label\n",
    "            metrics =  MulticlassMetrics(predictionAndLabels)\n",
    "            confusion = metrics.confusionMatrix()            \n",
    "            print(\"Confusion matrix: \\n\" , confusion)\n",
    "            \n",
    "            print(\"--------------------\")\n",
    "\n",
    "            newRow = spark.createDataFrame([(model,label,mode,param_print,accuracy,1-accuracy)], schema)\n",
    "            evalDF = evalDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "42a22d6e-f50b-48d5-87dc-667e1f078f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "|model             |target label|mode         |param_config                                                                                                                                                                                                                                                                                                                  |accuracy  |test error|\n",
      "+------------------+------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "|LogisticRegression|DEP_DEL15   |Unconditional|elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)|0.8629238 |0.13707621|\n",
      "|LogisticRegression|ARR_DEL15   |Unconditional|elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)|0.86149937|0.13850063|\n",
      "|LogisticRegression|ARR_DEL15   |Conditional  |elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 1.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 20)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.7)|0.86149937|0.13850063|\n",
      "+------------------+------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evalDF.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ead53265-6305-4ed0-a9e9-d6a150663c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
