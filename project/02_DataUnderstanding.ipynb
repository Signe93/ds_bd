{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\r\n",
    "import sys\r\n",
    "assert sys.version_info >= (3, 5)\r\n",
    "\r\n",
    "# Scikit-Learn ≥0.20 is required\r\n",
    "import sklearn\r\n",
    "assert sklearn.__version__ >= \"0.20\"\r\n",
    "\r\n",
    "# Common imports\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# To plot pretty figures\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "mpl.rc('axes', labelsize=14)\r\n",
    "mpl.rc('xtick', labelsize=12)\r\n",
    "mpl.rc('ytick', labelsize=12)\r\n",
    "\r\n",
    "# Where to save the figures\r\n",
    "PROJECT_ROOT_DIR = \".\"\r\n",
    "CHAPTER_ID = \"end_to_end_project\"\r\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\r\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\r\n",
    "\r\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\r\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\r\n",
    "    print(\"Saving figure\", fig_id)\r\n",
    "    if tight_layout:\r\n",
    "        plt.tight_layout()\r\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\r\n",
    "\r\n",
    "# for pretty printing\r\n",
    "def printDf(sprkDF): \r\n",
    "    newdf = sprkDF.toPandas()\r\n",
    "    from IPython.display import display, HTML\r\n",
    "    return HTML(newdf.to_html())\r\n",
    "\r\n",
    "# Ignore useless warnings (see SciPy issue #5998)\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\r\n",
    "\r\n",
    "# Spark libs\r\n",
    "from pyspark.sql.session import SparkSession\r\n",
    "\r\n",
    "# helper functions\r\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/Flight_Delay_Jan_2020_ontime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"FlightDataStatistics\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(inputFile) \\\n",
    "        .withColumn(\"DIVERTED_BOOL\", expr(\"DIVERTED\").cast(BooleanType())) \\\n",
    "        .withColumn(\"CANCELLED_BOOL\", expr(\"CANCELLED\").cast(BooleanType())) \\\n",
    "        .withColumn(\"DEP_DEL15_BOOL\", expr(\"DEP_DEL15\").cast(BooleanType())) \\\n",
    "        .withColumn(\"ARR_DEL15_BOOL\", expr(\"ARR_DEL15\").cast(BooleanType())) \\\n",
    "        \n",
    "pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anzahl Spalten\r\n",
    "anzahlSpalten = len(pysparkDF.columns)\r\n",
    "print(\"Der Datensatz enthält \" + str(anzahlSpalten) + \" Spalten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anzahl Zeilen\r\n",
    "anzahlZeilen = pysparkDF.count()\r\n",
    "print(\"Der Datensatz enthält \" + str(anzahlZeilen) + \" Zeilen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether OP_CARRIER_FL_NUM is merely running id for flights or rather encoding specific trims (e.g. Istanbul -> New York)\n",
    "pysparkDF.groupby('OP_CARRIER_FL_NUM').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# A tail number refers to an identification number painted on an aircraft, frequently on the tail.\n",
    "# Check amount of flights per plane\n",
    "pysparkDF.groupby('TAIL_NUM').count().show()\n",
    "\n",
    "# Check average flights per plane per year\n",
    "pysparkDF.groupby('TAIL_NUM').count().agg(F.mean('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether ORIGIN_AIRPORT_ID is 1:1 mapping onto ORIGIN\n",
    "pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN']).count().count() == pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN']).count().dropDuplicates(['ORIGIN_AIRPORT_ID']).count()\n",
    "\n",
    "# -> ORIGIN_AIRPORT_ID is string indexing ORIGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether ORIGIN_AIRPORT_ID is 1:1 mapping onto ORIGIN_AIRPORT_SEQ_ID\n",
    "pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID']).count().count() == pysparkDF.groupby(['ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID']).count().dropDuplicates(['ORIGIN_AIRPORT_ID']).count()\n",
    "\n",
    "# -> ORIGIN_AIRPORT_ID is 1:1 mapping to ORIGIN_AIRPORT_SEQ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether DEST_AIRPORT_ID is 1:1 mapping onto DEST\n",
    "pysparkDF.groupby(['DEST_AIRPORT_ID','DEST']).count().count() == pysparkDF.groupby(['DEST_AIRPORT_ID','DEST']).count().dropDuplicates(['DEST_AIRPORT_ID']).count()\n",
    "\n",
    "# -> DEST_AIRPORT_ID is string indexing DEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether DEST_AIRPORT_ID is 1:1 mapping onto DEST_AIRPORT_SEQ_ID\n",
    "pysparkDF.groupby(['DEST_AIRPORT_ID','DEST_AIRPORT_SEQ_ID']).count().count() == pysparkDF.groupby(['DEST_AIRPORT_ID','DEST_AIRPORT_SEQ_ID']).count().dropDuplicates(['DEST_AIRPORT_SEQ_ID']).count()\n",
    "\n",
    "# -> DEST_AIRPORT_ID is 1:1 mapping to ORIGIN_AIRPORT_SEQ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether OP_UNIQUE_CARRIER is 1:1 mapping onto OP_CARRIER\n",
    "pysparkDF.groupby(['OP_UNIQUE_CARRIER','OP_CARRIER']).count().count() == pysparkDF.groupby(['OP_UNIQUE_CARRIER','OP_CARRIER']).count().dropDuplicates(['OP_UNIQUE_CARRIER']).count()\n",
    "\n",
    "# -> OP_UNIQUE_CARRIER is 1:1 mapping to OP_CARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA UNDERSTANDING!\n",
    "# Check whether TAIL_NUM is 1:1 mapping onto OP_CARRIER_FL_NUM\n",
    "pysparkDF.groupby(['TAIL_NUM','OP_CARRIER_FL_NUM']).count().count() == pysparkDF.groupby(['TAIL_NUM','OP_CARRIER_FL_NUM']).count().dropDuplicates(['OP_CARRIER_FL_NUM']).count()\n",
    "\n",
    "# -> TAIL_NUM is not 1:1 mapping to OP_CARRIER_FL_NUM -> One distinct plane can fly multiple routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF =pysparkDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Übersicht der Kennzahlen wie Anzahl der Einträge, Mittelwert, Minimum und Maximum je Spalte kann durch den Befehl 'pandasDF.describe()' erzeugt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove faulty features\n",
    "Bei der Spalte \"_c21\" handelt es sich um eine leere Spalte. Die Spalte enthält keine Daten und kann somit entfernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pysparkDF = pysparkDF.drop('_c21')\r\n",
    "#pysparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove records containing NULL values\n",
    "Der Datensatz enthält Felder mit NULL Werten. Diese werden für die Auswertung enfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF_nonull = pysparkDF.dropna()\n",
    "f\"Removed {pysparkDF.count()-pysparkDF_nonull.count()} records containing NULL values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der Einträge je Wert einer Spalte kann mit Hilfe der Funktion 'DataFrame('Kategorie').value_counts()' berechnet werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl an pünktlichen (0) und verspäteten (1) Flügen beim Abflug\")\r\n",
    "print(pandasDF['DEP_DEL15'].value_counts())\r\n",
    "\r\n",
    "print(\"Anzahl an pünktlichen (0) und verspäteten (1) Flügen bei der Ankuft\")\r\n",
    "print(pandasDF['ARR_DEL15'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkDF.drop('OP_CARRIER','DEST','DEST_AIRPORT_SEQ_ID','OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN', '_c21', 'TAIL_NUM', 'OP_CARRIER_FL_NUM').toPandas().hist(bins=50, figsize=(20,15))\r\n",
    "save_fig(\"attribute_histogram_plots\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Count of Flights Delayed and on Time\r\n",
    "\r\n",
    "# Create bars for filtered value Delayed (1) and on time (0)\r\n",
    "ax = pysparkDF.groupby([\"OP_CARRIER\",\"DEP_DEL15\"]).count().filter(pysparkDF.DEP_DEL15 == 0).toPandas().sort_values(by=\"OP_CARRIER\",ascending=True).plot.bar(stacked=True, x=\"OP_CARRIER\", color='Blue', label='Not Delayed')\r\n",
    "pysparkDF.groupby([\"OP_CARRIER\",\"DEP_DEL15\"]).count().filter(pysparkDF.DEP_DEL15 == 1).toPandas().sort_values(by=\"OP_CARRIER\",ascending=True).plot.bar(stacked=True, x=\"OP_CARRIER\", color='Orange', label='Delayed', ax=ax )\r\n",
    "\r\n",
    "\r\n",
    "plt.ylabel(\"Count Flight\")\r\n",
    "\r\n",
    "save_fig(\"stacked_bar\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF.groupby(by=\"OP_CARRIER\")[['OP_CARRIER','CANCELLED', \"DIVERTED\",'ARR_DEL15']].sum().plot.bar()\r\n",
    "\r\n",
    "save_fig(\"grouped_bar\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar Chart 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\r\n",
    "\r\n",
    "pandasDF.head()\r\n",
    "\r\n",
    "x,y = 'DAY_OF_WEEK', 'DEP_DEL15'\r\n",
    "\r\n",
    "#Normalize Counts\r\n",
    "(pandasDF\r\n",
    ".groupby(x)[y]\r\n",
    ".value_counts(normalize=True)\r\n",
    ".mul(100)\r\n",
    ".rename('percentage')\r\n",
    ".reset_index()\r\n",
    ".pipe((sns.catplot,'data'), x= 'DAY_OF_WEEK',y='percentage',hue=y, kind='bar'))\r\n",
    "\r\n",
    "plt.legend(loc='upper right')\r\n",
    "save_fig(\"normalized_bar\")\r\n",
    "# show the graph\r\n",
    "plt.show()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie Chart - Show Cancelled Flights as pecentage per day of week\r\n",
    "pysparkDF.groupby(\"DAY_OF_WEEK\", \"CANCELLED\").count().filter(pysparkDF.CANCELLED==1).toPandas().plot.pie(y=\"DAY_OF_WEEK\", autopct=\"%.1f%%\", legend=False)\r\n",
    "plt.suptitle(\"Cancelled Flights\")\r\n",
    "save_fig(\"pie_chart\")\r\n",
    "plt.show()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Correlation of attributes\r\n",
    "import matplotlib.patches as mpatches\r\n",
    "\r\n",
    "plt.figure(figsize = (12, 10))\r\n",
    "sns.heatmap(pandasDF.corr(), annot = True, cmap = 'vlag')\r\n",
    "save_fig(\"correlation_heatmap\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pysparkDF.select(\"OP_CARRIER_AIRLINE_ID\", \"ORIGIN_AIRPORT_ID\").filter(pysparkDF.ARR_DEL15 != 0).toPandas().plot.scatter(x='OP_CARRIER_AIRLINE_ID', y='ORIGIN_AIRPORT_ID', color='DarkBlue', label='ARR_DEL15 1')\r\n",
    "pysparkDF.select(\"OP_CARRIER_AIRLINE_ID\", \"ORIGIN_AIRPORT_ID\").filter(pysparkDF.DEP_DEL15 != 0).toPandas().plot.scatter(x='OP_CARRIER_AIRLINE_ID', y='ORIGIN_AIRPORT_ID', color='Yellow', label='DEP_DEL15 1', ax=ax)\r\n",
    "\r\n",
    "save_fig(\"scatter_plots\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\r\n",
    "scatter_matrix(pysparkDF.select(\"OP_CARRIER_AIRLINE_ID\", \"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\", \"DEP_DEL15\", \"Distance\").toPandas(), alpha=0.2, figsize=(30, 30), diagonal='kde')\r\n",
    "save_fig(\"scatter_matrix\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6478c44a3317eea7880644b1e90701767b1772da5d4d1e50fe9680e69436a5ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}