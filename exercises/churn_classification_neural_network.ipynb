{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ds_db-.venv",
   "display_name": "Python 3.7.5  ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classification via Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.helper_functions import translate_to_file_string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler, VectorIndexer, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"ChurnClustering\")\n",
    "       .getOrCreate())\n",
    "# create a DataFrame using an ifered Schema \n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .option(\"delimiter\", \";\") \\\n",
    "       .csv(inputFile)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preperation\n",
    "splits = df.randomSplit([0.6, 0.4 ], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "   \n",
    "# Transform labels into index\n",
    "labelIndexer = StringIndexer(inputCol=\"LEAVE\", outputCol=\"label\").fit(df)\n",
    "collegeIndexer = StringIndexer().setInputCol(\"COLLEGE\").setOutputCol(\"COLLEGE_NUM\")\n",
    "satIndexer = StringIndexer().setInputCol(\"REPORTED_SATISFACTION\").setOutputCol(\"REPORTED_SATISFACTION_NUM\")\n",
    "usageIndexer = StringIndexer().setInputCol(\"REPORTED_USAGE_LEVEL\").setOutputCol(\"REPORTED_USAGE_LEVEL_NUM\")\n",
    "changeIndexer = StringIndexer().setInputCol(\"CONSIDERING_CHANGE_OF_PLAN\").setOutputCol(\"CONSIDERING_CHANGE_OF_PLAN_NUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build feature vector\n",
    "featureCols = df.columns.copy()\n",
    "featureCols.remove(\"LEAVE\")\n",
    "featureCols.remove(\"COLLEGE\")\n",
    "featureCols.remove(\"REPORTED_SATISFACTION\")\n",
    "featureCols.remove(\"REPORTED_USAGE_LEVEL\")\n",
    "featureCols.remove(\"CONSIDERING_CHANGE_OF_PLAN\")\n",
    "featureCols = featureCols +[\"COLLEGE_NUM\",\"REPORTED_SATISFACTION_NUM\",\"REPORTED_USAGE_LEVEL_NUM\",\"CONSIDERING_CHANGE_OF_PLAN_NUM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols))\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "predConverter = IndexToString(inputCol=\"prediction\",outputCol=\"predictedLabel\",labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MultilayerPerceptronClassifier(seed=1234, featuresCol=\"scaledFeatures\")\n",
    "   \n",
    "# build network parameters grid\n",
    "# TODO add change the params \n",
    "\n",
    "paramGrid =  ParamGridBuilder().addGrid(nn.layers, [[ 11, 12, 5, 2 ]]) \\\n",
    "\t\t\t\t.addGrid(nn.blockSize,  [128 ]) \\\n",
    "                .addGrid(nn.maxIter,[ 100 ] )\\\n",
    "\t\t\t\t.addGrid(nn.stepSize, [ 0.3 ])\\\n",
    "\t\t\t\t.addGrid(nn.tol, [ 0.05 ]) \\\n",
    "\t\t\t\t.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages= [labelIndexer, collegeIndexer, satIndexer,\n",
    "\t\t\t\tusageIndexer, changeIndexer, assembler, scaler, nn, predConverter ])\n",
    "\n",
    "\n",
    "evaluator =  BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator,estimatorParamMaps=paramGrid,numFolds=2, parallelism=2)\n",
    "\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel.stages[7]\n",
    "print(\"Layers: \" , bestModel.layers)\n",
    "print(bestModel.explainParams())\n",
    "  \n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "predictions.show()\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = \" ,(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ]
}